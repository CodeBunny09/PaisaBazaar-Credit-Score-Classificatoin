








# 📦 Importing Required Libraries

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Suppress warnings
import warnings
warnings.filterwarnings("ignore")

# Display full column width and set float format
pd.set_option('display.max_columns', None)
pd.set_option('display.float_format', '{:.2f}'.format)





# Load the dataset
file_path = 'data/raw/dataset-2.csv'
df = pd.read_csv(file_path)

# Preview the first few rows
df.head()





# 🧮 Check dataset shape
print(f"Number of Rows: {df.shape[0]}")
print(f"Number of Columns: {df.shape[1]}")

# 🔁 Check for duplicates
duplicate_rows = df.duplicated().sum()
print(f"Number of duplicate rows: {duplicate_rows}")





# 🧾 Data type and non-null counts
df.info()





# 📌 Check for real and disguised missing values
import numpy as np

# Convert all object-type columns to strings for safe processing
df_obj = df.select_dtypes(include='object')

# Check for known placeholders
placeholders = ['NA', 'N/A', 'null', 'NULL', '', ' ']
null_mask = df_obj.apply(lambda x: x.isin(placeholders)).sum()

# Regular null check
regular_nulls = df.isnull().sum()

# Combine and display
null_report = pd.DataFrame({
    'Regular_Nulls': regular_nulls,
    'Placeholder_Nulls': null_mask
})
null_report['Total_Nulls'] = null_report['Regular_Nulls'] + null_report['Placeholder_Nulls']
null_report = null_report[null_report['Total_Nulls'] > 0]
null_report





# 📊 Statistical overview of numerical columns
df.describe().T





# 🎯 Unique values in each column
df.nunique().sort_values(ascending=False)





# 📉 Check for missing/null values
df.isnull().sum().sort_values(ascending=False)





# 🔁 Drop duplicates based on all columns
df_cleaned = df.drop_duplicates()

# 🔢 Check if duplicate IDs exist
print("Duplicate ID count:", df_cleaned.duplicated(subset='ID').sum())

# ✅ Confirm shape after cleaning
df_cleaned.shape





# Columns to drop
cols_to_drop = ['ID', 'Customer_ID', 'SSN', 'Name']

df_cleaned.drop(columns=cols_to_drop, inplace=True)

# ✅ Confirm shape
df_cleaned.shape





# Display column data types
df_cleaned.dtypes.value_counts()





# Show unique values (up to 10) for object-type columns
for col in df_cleaned.select_dtypes(include='object').columns:
    print(f"\n🔹 {col} — {df_cleaned[col].nunique()} unique values")
    print(df_cleaned[col].unique()[:10])





# Ordinal mapping for credit mix and score
ordinal_maps = {
    'Credit_Mix': {'Bad': 0, 'Standard': 1, 'Good': 2},
    'Credit_Score': {'Poor': 0, 'Standard': 1, 'Good': 2},
    'Payment_of_Min_Amount': {'No': 0, 'NM': 1, 'Yes': 2}
}

# Apply mappings
df_cleaned.replace(ordinal_maps, inplace=True)

# Confirm transformation
df_cleaned[['Credit_Mix', 'Credit_Score', 'Payment_of_Min_Amount']].head()





# One-hot encoding selected nominal columns
one_hot_cols = ['Occupation', 'Payment_Behaviour']
df_cleaned = pd.get_dummies(df_cleaned, columns=one_hot_cols, drop_first=True)

# Check shape and confirm new columns
print(f"✅ Data shape after One-Hot Encoding: {df_cleaned.shape}")
df_cleaned.columns[-10:]  # Show last 10 columns to confirm encoding





from sklearn.preprocessing import MultiLabelBinarizer

# Convert comma-separated string to list
df_cleaned['Type_of_Loan'] = df_cleaned['Type_of_Loan'].apply(lambda x: [i.strip() for i in x.split(',')])

# Apply MultiLabelBinarizer
mlb = MultiLabelBinarizer()
loan_encoded = pd.DataFrame(mlb.fit_transform(df_cleaned['Type_of_Loan']), 
                            columns=['Loan_' + col for col in mlb.classes_])

# Concatenate and drop original column
df_cleaned = pd.concat([df_cleaned.drop('Type_of_Loan', axis=1), loan_encoded], axis=1)

# Check result
print(f"✅ Shape after adding Loan columns: {df_cleaned.shape}")
loan_encoded.columns





# Create processed data directory if not exists
import os

processed_dir = "data/processed"
os.makedirs(processed_dir, exist_ok=True)

# Save as CSV
processed_path = os.path.join(processed_dir, "cleaned_dataset.csv")
df_cleaned.to_csv(processed_path, index=False)

print(f"✅ Cleaned data saved to:\n{processed_path}")





# 📊 Chart 1: Distribution of Credit Scores

import seaborn as sns
import matplotlib.pyplot as plt

plt.figure(figsize=(8, 5))
ax = sns.countplot(data=df, x='Credit_Score', palette='Set2')
plt.title('Distribution of Credit Scores', fontsize=14)
plt.xlabel('Credit Score Category')
plt.ylabel('Count')
plt.xticks(rotation=0)
plt.grid(axis='y')
plt.tight_layout()
plt.show()

# 🔍 Output X & Y axis variables
print("X-axis (Categories):", df['Credit_Score'].unique())
print("Y-axis (Counts):", df['Credit_Score'].value_counts().to_dict())






# 📊 Chart 2: Age Distribution

plt.figure(figsize=(10, 6))
ax = sns.histplot(data=df, x='Age', bins=30, kde=True, color='skyblue')
plt.title('Distribution of Customer Age', fontsize=14)
plt.xlabel('Age')
plt.ylabel('Number of Customers')
plt.grid(axis='y')
plt.tight_layout()
plt.show()

# 🔍 Output X & Y axis info
print("X-axis (Age Range):", f"{df['Age'].min()} to {df['Age'].max()}")
print("Y-axis: Count of customers in each age bin")





# 📊 Chart 3: Annual Income Distribution

plt.figure(figsize=(10, 6))
ax = sns.histplot(data=df, x='Annual_Income', bins=40, kde=True, color='orange')
plt.title('Distribution of Annual Income', fontsize=14)
plt.xlabel('Annual Income (in USD)')
plt.ylabel('Number of Customers')
plt.grid(axis='y')
plt.tight_layout()
plt.show()

# 🔍 Output X & Y axis info
print("X-axis (Income Range):", f"{df['Annual_Income'].min():,.2f} to {df['Annual_Income'].max():,.2f}")
print("Y-axis: Count of customers in each income bin")





# 📊 Chart 4: Number of Loans by Credit Score Category

plt.figure(figsize=(9, 6))
ax = sns.boxplot(data=df, x='Credit_Score', y='Num_of_Loan', palette='Set2')
plt.title('📦 Number of Loans vs Credit Score')
plt.xlabel('Credit Score')
plt.ylabel('Number of Loans')
plt.grid(axis='y')
plt.tight_layout()
plt.show()

# 🔍 Output X & Y axis info
print("X-axis Categories:", df['Credit_Score'].unique())
print("Y-axis: Number of Loans per Customer (spread per credit group)")





# 📊 Chart 5: Credit Utilization Ratio vs Credit Score

plt.figure(figsize=(9, 6))
ax = sns.boxplot(data=df, x='Credit_Score', y='Credit_Utilization_Ratio', palette='pastel')
plt.title('📦 Credit Utilization Ratio vs Credit Score')
plt.xlabel('Credit Score')
plt.ylabel('Credit Utilization Ratio (%)')
plt.grid(axis='y')
plt.tight_layout()
plt.show()

# 🔍 Output X & Y axis info
print("X-axis Categories:", df['Credit_Score'].unique())
print("Y-axis: Credit Utilization Ratio (percentage per credit group)")





# 📊 Chart 6: Monthly Balance vs Credit Mix

plt.figure(figsize=(9, 6))
ax = sns.boxplot(data=df, x='Credit_Mix', y='Monthly_Balance', palette='Set2')
plt.title('💰 Monthly Balance vs Credit Mix')
plt.xlabel('Credit Mix Type')
plt.ylabel('Monthly Balance')
plt.grid(axis='y')
plt.tight_layout()
plt.show()

# 🔍 Output X & Y axis info
print("X-axis Categories:", df['Credit_Mix'].unique())
print("Y-axis: Monthly Balance (Available Monthly Cash Flow)")





# 📊 Chart 7: Delay from Due Date vs Credit Score

plt.figure(figsize=(9, 6))
ax = sns.violinplot(data=df, x='Credit_Score', y='Delay_from_due_date', palette='pastel')
plt.title('⏳ Payment Delay vs Credit Score')
plt.xlabel('Credit Score Category')
plt.ylabel('Delay from Due Date (Days)')
plt.grid(axis='y')
plt.tight_layout()
plt.show()

# 🔍 Output X & Y axis info
print("X-axis Categories:", df['Credit_Score'].unique())
print("Y-axis: Delay from Due Date (in days)")





# 📊 Chart 8: Credit Inquiries vs Credit Score

plt.figure(figsize=(9, 6))
ax = sns.boxplot(data=df, x='Credit_Score', y='Num_Credit_Inquiries', palette='coolwarm')
plt.title('📂 Credit Inquiries vs Credit Score')
plt.xlabel('Credit Score Category')
plt.ylabel('Number of Credit Inquiries')
plt.grid(axis='y')
plt.tight_layout()
plt.show()

# 🔍 Output X & Y axis info
print("X-axis Categories:", df['Credit_Score'].unique())
print("Y-axis: Number of Credit Inquiries")





# 📊 Chart 9: Payment Behaviour Distribution

plt.figure(figsize=(12, 6))
ax = sns.countplot(data=df, x='Payment_Behaviour', order=df['Payment_Behaviour'].value_counts().index, palette='viridis')
plt.title('💸 Distribution of Payment Behaviour')
plt.xlabel('Payment Behaviour Type')
plt.ylabel('Number of Customers')
plt.xticks(rotation=45)
plt.grid(axis='y')
plt.tight_layout()
plt.show()

# 🔍 Output X & Y axis info
print("X-axis Categories:", df['Payment_Behaviour'].unique())
print("Y-axis: Count of Customers per Payment Behaviour")





# 📊 Chart 10: Credit Score vs. Delayed Payments

plt.figure(figsize=(10, 6))
ax = sns.boxplot(data=df, x='Credit_Score', y='Num_of_Delayed_Payment', palette='Set2')
plt.title('📉 Delayed Payments Across Credit Score Categories')
plt.xlabel('Credit Score')
plt.ylabel('Number of Delayed Payments')
plt.grid(axis='y')
plt.tight_layout()
plt.show()

# 🔍 Output X & Y axis info
print("X-axis Categories:", df['Credit_Score'].unique())
print("Y-axis: Distribution of Delayed Payments for each Credit Score category")





# 📊 Chart 11: Credit History Age vs. Credit Score

plt.figure(figsize=(10, 6))
ax = sns.boxplot(data=df, x='Credit_Score', y='Credit_History_Age', palette='Set3')
plt.title('📘 Credit History Age Distribution by Credit Score')
plt.xlabel('Credit Score')
plt.ylabel('Credit History Age (in months)')
plt.grid(axis='y')
plt.tight_layout()
plt.show()

# 🔍 Output X & Y axis info
print("X-axis Categories:", df['Credit_Score'].unique())
print("Y-axis: Distribution of Credit History Age for each Credit Score category")





# 📊 Chart 12: Number of Credit Cards vs. Credit Score

plt.figure(figsize=(10, 6))
ax = sns.boxplot(data=df, x='Credit_Score', y='Num_Credit_Card', palette='YlGnBu')
plt.title('💳 Number of Credit Cards by Credit Score')
plt.xlabel('Credit Score')
plt.ylabel('Number of Credit Cards')
plt.grid(axis='y')
plt.tight_layout()
plt.show()

# 🔍 Output X & Y axis info
print("X-axis Categories:", df['Credit_Score'].unique())
print("Y-axis: Distribution of Number of Credit Cards for each Credit Score category")





# 💰 Chart 13: Monthly Investment vs. Credit Score

plt.figure(figsize=(10, 6))
ax = sns.boxplot(data=df, x='Credit_Score', y='Amount_invested_monthly', palette='Pastel1')
plt.title('📈 Monthly Investment by Credit Score')
plt.xlabel('Credit Score')
plt.ylabel('Monthly Amount Invested')
plt.grid(axis='y')
plt.tight_layout()
plt.show()

# 🔍 Output X & Y axis info
print("X-axis Categories:", df['Credit_Score'].unique())
print("Y-axis: Distribution of Amount Invested Monthly for each Credit Score category")





# 🔥 Chart 14: Correlation Heatmap

# Select only numeric columns
numeric_df = df.select_dtypes(include=['float64', 'int64'])

# Compute the correlation matrix
corr_matrix = numeric_df.corr()

plt.figure(figsize=(14, 10))
sns.heatmap(corr_matrix, annot=True, fmt=".2f", cmap='coolwarm', square=True, linewidths=0.5, cbar_kws={"shrink": 0.8})
plt.title('🔥 Correlation Heatmap of Numerical Features')
plt.tight_layout()
plt.show()

# 🔍 Output axis info
print("X and Y axes show correlations between all numeric variables in the dataset")





# 🧩 Chart 15: Credit Mix vs Credit Score

plt.figure(figsize=(8, 5))
sns.countplot(data=df, x='Credit_Mix', hue='Credit_Score', palette='Set2')
plt.title('🧩 Credit Mix vs Credit Score')
plt.xlabel('Credit Mix')
plt.ylabel('Count')
plt.xticks(rotation=0)
plt.legend(title='Credit Score')
plt.tight_layout()
plt.show()

# 🔍 Output axis info
print("X-axis: Credit_Mix (categories like Good, Standard, Bad)")
print("Y-axis: Count of customers")
print("Hue: Credit_Score (Good, Standard, Poor)")





# 💰 Chart 16: Distribution of Monthly Balance by Credit Score

plt.figure(figsize=(10, 5))
sns.kdeplot(data=df, x='Monthly_Balance', hue='Credit_Score', fill=True, palette='Set1', alpha=0.6)
plt.title('💰 Distribution of Monthly Balance by Credit Score')
plt.xlabel('Monthly Balance')
plt.ylabel('Density')
plt.legend(title='Credit Score')
plt.tight_layout()
plt.show()

# 🔍 Output axis info
print("X-axis: Monthly_Balance (continuous variable in dollars)")
print("Y-axis: Density (distribution curve)")
print("Hue: Credit_Score (Good, Standard, Poor)")





# 🧾 Chart 17: Number of Loans vs Credit Score

plt.figure(figsize=(8, 5))
sns.boxplot(data=df, x='Credit_Score', y='Num_of_Loan', palette='Set2')
plt.title('🧾 Number of Loans vs Credit Score')
plt.xlabel('Credit Score')
plt.ylabel('Number of Loans')
plt.tight_layout()
plt.show()

# 🔍 Output axis info
print("X-axis: Credit_Score (categorical variable: Good, Standard, Poor)")
print("Y-axis: Num_of_Loan (number of different loan types a customer has)")





# 🧾 Chart 18: Credit Mix Distribution Across Credit Scores

plt.figure(figsize=(8, 5))
sns.countplot(data=df, x='Credit_Score', hue='Credit_Mix', palette='Set1')
plt.title('🧾 Credit Mix Distribution Across Credit Scores')
plt.xlabel('Credit Score')
plt.ylabel('Count')
plt.legend(title='Credit Mix')
plt.tight_layout()
plt.show()

# 🔍 Output axis info
print("X-axis: Credit_Score (Good, Standard, Poor)")
print("Hue: Credit_Mix (Standard, Good, Bad)")
print("Y-axis: Number of Customers")





# 💰 Chart 19: Annual Income vs Monthly In-hand Salary (Colored by Credit Score)

plt.figure(figsize=(10, 6))
sns.scatterplot(
    data=df, 
    x='Annual_Income', 
    y='Monthly_Inhand_Salary', 
    hue='Credit_Score', 
    alpha=0.6,
    palette='Set2'
)
plt.title('💰 Annual Income vs Monthly In-hand Salary (Colored by Credit Score)')
plt.xlabel('Annual Income')
plt.ylabel('Monthly In-hand Salary')
plt.legend(title='Credit Score')
plt.tight_layout()
plt.show()

# 🔍 Output axis info
print("X-axis: Annual_Income")
print("Y-axis: Monthly_Inhand_Salary")
print("Hue: Credit_Score (Good, Standard, Poor)")





# 📊 Chart 20: Distribution of Number of Delayed Payments by Credit Score

plt.figure(figsize=(10, 6))
sns.histplot(
    data=df,
    x='Num_of_Delayed_Payment',
    hue='Credit_Score',
    bins=30,
    kde=True,
    palette='Set1',
    multiple='stack'
)
plt.title('📊 Distribution of Number of Delayed Payments by Credit Score')
plt.xlabel('Number of Delayed Payments')
plt.ylabel('Count')
plt.tight_layout()
plt.show()

# 🔍 Output axis info
print("X-axis: Num_of_Delayed_Payment")
print("Y-axis: Count of Customers")
print("Hue: Credit_Score (Good, Standard, Poor)")











from scipy.stats import f_oneway

# Get income data by credit score class
income_good = df_cleaned[df_cleaned['Credit_Score'] == 2]['Annual_Income']
income_standard = df_cleaned[df_cleaned['Credit_Score'] == 1]['Annual_Income']
income_poor = df_cleaned[df_cleaned['Credit_Score'] == 0]['Annual_Income']

# Perform One-Way ANOVA
f_stat, p_val = f_oneway(income_good, income_standard, income_poor)

print("📈 F-statistic:", round(f_stat, 4))
print("📊 P-value:", round(p_val, 6))






from scipy.stats import chi2_contingency
import pandas as pd

# Bin credit card counts
df_cleaned['CreditCard_Bin'] = pd.cut(df_cleaned['Num_Credit_Card'], bins=[-1, 2, 5, 8, 12], labels=['Low', 'Moderate', 'High', 'Very High'])

# Create contingency table
contingency_table = pd.crosstab(df_cleaned['CreditCard_Bin'], df_cleaned['Credit_Score'])

# Run chi-square test
chi2_stat, p_val, dof, expected = chi2_contingency(contingency_table)

print("🔢 Chi-Square Statistic:", round(chi2_stat, 4))
print("📊 P-value:", round(p_val, 6))
print("📐 Degrees of Freedom:", dof)





# Get all columns related to occupation
occupation_columns = [col for col in df_cleaned.columns if col.startswith("Occupation_")]

# Reconstruct the original Occupation category
df_cleaned['Occupation'] = df_cleaned[occupation_columns].idxmax(axis=1).str.replace('Occupation_', '')

from scipy.stats import f_oneway

# Create groups for ANOVA test
groups = [df_cleaned[df_cleaned['Occupation'] == occ]['Delay_from_due_date'] for occ in df_cleaned['Occupation'].unique()]

# Run One-Way ANOVA
f_stat, p_val = f_oneway(*groups)

print("📈 F-statistic:", round(f_stat, 4))
print("📊 P-value:", round(p_val, 6))








# 📦 Imports
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split

# 🔽 Load cleaned data
df = pd.read_csv('data/processed/cleaned_dataset.csv')

# 🏷️ Encode categorical features
categorical_cols = df.select_dtypes(include='object').columns.tolist()
print("Categorical columns to encode:", categorical_cols)

# 🔄 Label Encoding (since all are ordinal/nominal, not high-cardinality text)
label_encoders = {}
for col in categorical_cols:
    le = LabelEncoder()
    df[col] = le.fit_transform(df[col])
    label_encoders[col] = le

# ✅ Split features and target
X = df.drop(columns=['Credit_Score'])  # 🔍 Features
y = df['Credit_Score']                 # 🎯 Target

# 🔀 Train-Test Split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 🔍 Show shape and sample
print("✅ Shapes -> X_train:", X_train.shape, "X_test:", X_test.shape)
X_train.head()





from sklearn.preprocessing import StandardScaler

# 📊 Identify numeric columns
numeric_cols = X.select_dtypes(include=['float64', 'int64']).columns.tolist()
print("🧮 Numeric columns to scale:", numeric_cols[:10], '...')

# 🧼 Apply standard scaling
scaler = StandardScaler()
X_train_scaled = X_train.copy()
X_test_scaled = X_test.copy()
X_train_scaled[numeric_cols] = scaler.fit_transform(X_train[numeric_cols])
X_test_scaled[numeric_cols] = scaler.transform(X_test[numeric_cols])

# ✅ Confirm scaling
X_train_scaled.head()





from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score

# ⚙️ Define models
models = {
    "Logistic Regression": LogisticRegression(max_iter=1000),
    "Random Forest": RandomForestClassifier(n_estimators=100, random_state=42),
    "XGBoost": XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=42)
}

# 📊 Train and Evaluate
for name, model in models.items():
    print(f"\n📌 Training: {name}")
    model.fit(X_train_scaled, y_train)
    y_pred = model.predict(X_test_scaled)

    print("✅ Accuracy:", accuracy_score(y_test, y_pred))
    print("📊 Classification Report:\n", classification_report(y_test, y_pred))
    print("🧩 Confusion Matrix:\n", confusion_matrix(y_test, y_pred))





# 🔍 Feature Importance Plot for XGBoost
import matplotlib.pyplot as plt
import seaborn as sns

# Refit XGBoost model just to extract feature importances
xgb_model = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=42)
xgb_model.fit(X_train_scaled, y_train)

# Plot top 20 features
plt.figure(figsize=(12, 8))
importance = xgb_model.feature_importances_
features = X_train_scaled.columns

# Create a DataFrame for better sorting and plotting
feature_df = pd.DataFrame({'Feature': features, 'Importance': importance})
top_features = feature_df.sort_values(by='Importance', ascending=False).head(20)

sns.barplot(x='Importance', y='Feature', data=top_features, palette='viridis')
plt.title('📌 Top 20 Important Features - XGBoost')
plt.tight_layout()
plt.show()

# 👇 Print variable names for reference
print("🔁 X-axis (Features):")
print(top_features['Feature'].tolist())
print("\n🟩 Y-axis: Feature Importance Score")





from sklearn.model_selection import GridSearchCV
from xgboost import XGBClassifier

# 🎛️ Define parameters to tune
param_grid = {
    'n_estimators': [100, 200],
    'max_depth': [4, 6, 8],
    'learning_rate': [0.05, 0.1],
    'subsample': [0.8, 1],
    'colsample_bytree': [0.8, 1]
}

# ⚙️ Grid Search
xgb = XGBClassifier(eval_metric='mlogloss', random_state=42)
grid_search = GridSearchCV(estimator=xgb, param_grid=param_grid, 
                           scoring='accuracy', cv=3, verbose=1, n_jobs=-1)

grid_search.fit(X_train_scaled, y_train)

# 📌 Best parameters and score
print("✅ Best Parameters:", grid_search.best_params_)
print("🎯 Best Cross-Validation Accuracy:", grid_search.best_score_)






# ✅ Retrain final XGBoost with best params
final_xgb = XGBClassifier(
    learning_rate=0.1,
    max_depth=8,
    n_estimators=200,
    subsample=0.8,
    colsample_bytree=1,
    eval_metric='mlogloss',
    random_state=42
)

final_xgb.fit(X_train_scaled, y_train)
y_pred_final = final_xgb.predict(X_test_scaled)

# 📊 Evaluation
print("✅ Final Accuracy:", accuracy_score(y_test, y_pred_final))
print("📊 Classification Report:\n", classification_report(y_test, y_pred_final))
print("🧩 Confusion Matrix:\n", confusion_matrix(y_test, y_pred_final))





from sklearn.preprocessing import label_binarize
from sklearn.metrics import roc_auc_score, roc_curve, auc
from sklearn.multiclass import OneVsRestClassifier
import matplotlib.pyplot as plt
from itertools import cycle
import numpy as np

# 🎯 Binarize y_test for multi-class ROC AUC
y_test_bin = label_binarize(y_test, classes=[0, 1, 2])
y_score = final_xgb.predict_proba(X_test_scaled)
n_classes = y_test_bin.shape[1]

# 📦 Initialize ROC containers
fpr = dict()
tpr = dict()
roc_auc = dict()
colors = cycle(['blue', 'red', 'green'])

# 🚀 Compute ROC curve and ROC area for each class
print("📊 Class-wise ROC AUC Scores:")
for i in range(n_classes):
    fpr[i], tpr[i], _ = roc_curve(y_test_bin[:, i], y_score[:, i])
    roc_auc[i] = auc(fpr[i], tpr[i])
    print(f"🔹 Class {i} - AUC: {roc_auc[i]:.4f}")

# 🌐 Compute micro and macro-average ROC AUC
fpr["micro"], tpr["micro"], _ = roc_curve(y_test_bin.ravel(), y_score.ravel())
roc_auc["micro"] = auc(fpr["micro"], tpr["micro"])
print(f"📉 Micro-average AUC: {roc_auc['micro']:.4f}")

# Macro-average (unweighted)
all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))
mean_tpr = np.zeros_like(all_fpr)
for i in range(n_classes):
    mean_tpr += np.interp(all_fpr, fpr[i], tpr[i])
mean_tpr /= n_classes

fpr["macro"] = all_fpr
tpr["macro"] = mean_tpr
roc_auc["macro"] = auc(fpr["macro"], tpr["macro"])
print(f"📈 Macro-average AUC: {roc_auc['macro']:.4f}")

# 🎨 Plot ROC curves
plt.figure(figsize=(10, 7))
for i, color in zip(range(n_classes), colors):
    plt.plot(fpr[i], tpr[i], color=color, lw=2,
             label=f"Class {i} (AUC = {roc_auc[i]:.2f})")

plt.plot(fpr["micro"], tpr["micro"], label=f"Micro-Average (AUC = {roc_auc['micro']:.2f})", 
         color='deeppink', linestyle=':', linewidth=3)

plt.plot(fpr["macro"], tpr["macro"], label=f"Macro-Average (AUC = {roc_auc['macro']:.2f})", 
         color='navy', linestyle=':', linewidth=3)

plt.plot([0, 1], [0, 1], 'k--', label='Chance (AUC = 0.50)')
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("🔍 ROC Curves for Final XGBoost Model (Multiclass)")
plt.legend(loc="lower right")
plt.grid(True)
plt.tight_layout()
plt.show()





from imblearn.over_sampling import SMOTE
from sklearn.pipeline import Pipeline

# ⚙️ Define SMOTE
smote = SMOTE(random_state=42)

# 🔁 Resample training data
X_train_smote, y_train_smote = smote.fit_resample(X_train_scaled, y_train)

print(f"🧮 Original Class Distribution:\n{y_train.value_counts()}")
print(f"\n📈 After SMOTE:\n{y_train_smote.value_counts()}")

# 🔁 Retrain XGBoost on balanced data
xgb_smote = XGBClassifier(
    colsample_bytree=1,
    learning_rate=0.1,
    max_depth=8,
    n_estimators=200,
    subsample=0.8,
    random_state=42
)

print("\n🚀 Training XGBoost on SMOTE data...")
xgb_smote.fit(X_train_smote, y_train_smote)
y_pred_smote = xgb_smote.predict(X_test_scaled)

# 📊 Evaluation
print("✅ Accuracy:", accuracy_score(y_test, y_pred_smote))
print("📊 Classification Report:\n", classification_report(y_test, y_pred_smote))
print("🧩 Confusion Matrix:\n", confusion_matrix(y_test, y_pred_smote))





from sklearn.utils.class_weight import compute_sample_weight

# 🧮 Compute class weights based on original labels
sample_weights = compute_sample_weight(class_weight='balanced', y=y_train)

# ♻️ Reinitialize the best XGBoost model
xgb_weighted = XGBClassifier(
    colsample_bytree=1,
    learning_rate=0.1,
    max_depth=8,
    n_estimators=200,
    subsample=0.8,
    random_state=42
)

# 🚀 Train with sample weights
print("🚀 Training XGBoost with class weights...")
xgb_weighted.fit(X_train_scaled, y_train, sample_weight=sample_weights)

# 🔍 Predict and evaluate
y_pred_weighted = xgb_weighted.predict(X_test_scaled)

print("✅ Accuracy:", accuracy_score(y_test, y_pred_weighted))
print("📊 Classification Report:\n", classification_report(y_test, y_pred_weighted))
print("🧩 Confusion Matrix:\n", confusion_matrix(y_test, y_pred_weighted))





import joblib
import os

# 📁 Ensure the models directory exists
os.makedirs("models", exist_ok=True)

# 💾 Save the best model (XGBoost with class weights)
joblib.dump(xgb_weighted, "models/best_model.pkl")
print("✅ Best model saved as 'models/best_model.pkl'")



